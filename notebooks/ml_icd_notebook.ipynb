{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_icd_simple_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qMF2QZq4-x9r",
        "mmEVBHnp-1MS",
        "sy6OcuP8zz7K",
        "hgwg-rVEPORv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmwCw6Mur1EV"
      },
      "source": [
        "# INTRO\n",
        "\n",
        "Imports and GDrive mounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhEeW9dDrRYD"
      },
      "source": [
        "#@title IMPORTS !!!\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import keras\n",
        "import time\n",
        "from itertools import zip_longest\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "def banner(\n",
        "    text: str, *, \n",
        "    length: int = 65, \n",
        "    frame_char: str = '#'\n",
        ") -> str:\n",
        "\tstext = ' %s ' % text\n",
        "\tmbanner = frame_char*2 + stext.center(length-4) + frame_char*2\n",
        "\tfframe = frame_char*length\n",
        "\teframe = frame_char*2 + ' '*(length-4) + frame_char*2\n",
        "\tbanner = f\"{fframe}\\n{eframe}\\n{mbanner}\\n{eframe}\\n{fframe}\\n\\n\"\n",
        "\treturn banner\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3lsmXGieV3s",
        "cellView": "form",
        "outputId": "9ddb56e6-8e28-40d7-b09d-489136075e6b"
      },
      "source": [
        "#@title !!! Drive mounting\n",
        "folder_name = \"224x224_full_6cat_data\" #@param {type:\"string\"}\n",
        "drive.mount(\"/content/drive\")\n",
        "base_path = f'/content/drive/My Drive/Studia/ML/{folder_name}/'\n",
        "models_path = '/content/drive/My Drive/Studia/ML/models/'\n",
        "print(base_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzwQsrjFWteB",
        "cellView": "form"
      },
      "source": [
        "#@title !!! Dir content loading \n",
        "dir_files = os.listdir(base_path)\n",
        "    class_file = list(filter(lambda file: \"classes\" in file, dir_files))[0]\n",
        "    test_files = list(filter(lambda file: \"test\" in file, dir_files))\n",
        "    train_files = list(filter(lambda file: \"train\" in file, dir_files))\n",
        "\n",
        "    with open(os.path.join(base_path, class_file)) as fjson:\n",
        "        classes = json.load(fjson)\n",
        "    \n",
        "    test_files.sort()\n",
        "    train_files.sort()\n",
        "\n",
        "print(banner(\"INITIAL DATA\", length=default_line_length))\n",
        "print(f\"{model_path=}\\n\")\n",
        "for trainf, testf in zip_longest([\"TRAIN_FILES\", train_files], [\"TEST_FILES\", test_files]):\n",
        "    print(f'{trainf}\\t\\t{testf or \"\"}'.center(default_line_length))\n",
        "\n",
        "print(\"CLASSES\")\n",
        "print(f\"{classes}\".center(default_line_length))\n",
        "print(f'Checkpoint saves: {\"True\" if tmp_save else \"False\"}\\n') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzzgqRh5r7Jh"
      },
      "source": [
        "# MODEL CREATION\n",
        "\n",
        "Model building: layers, compilation and summary\n",
        "\n",
        "`[CHOOSE ONE]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMF2QZq4-x9r"
      },
      "source": [
        "## > Simple model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLKotEMUa3vA"
      },
      "source": [
        "First: Train Model\n",
        "\n",
        "```\n",
        "[Layers]\n",
        "1. Flatten\n",
        "2. FC-128\n",
        "3. FC-19\n",
        " \n",
        "Input: 28x28 JPEG\n",
        " \n",
        "In sum: 100 epochs per batch\n",
        "dataset division: 80% training, 20% testing\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgXT4Zu5tvGH",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "11a60f45-13f6-477f-ccdb-9fe1f143975d"
      },
      "source": [
        "#@title Create model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# 1st layer\n",
        "# flatten output data\n",
        "model.add(layers.Flatten(input_shape=(28, 28, 3)))\n",
        "\n",
        "# 2nd layer\n",
        "model.add(layers.Dense( 128, activation='relu'))\n",
        "# 3rd layer\n",
        "model.add(layers.Dense( len(classes.keys()) ))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy']\n",
        ")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmEVBHnp-1MS"
      },
      "source": [
        "##  > Convolutional model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L6Oz_-t-6hE"
      },
      "source": [
        "```\n",
        "[Layers]\n",
        "1. Conv2D_64\n",
        "2. Maxpool2D, stride 2\n",
        "3. Conv2D_128\n",
        "4. Maxpool2D, stride 2\n",
        "5. Flatten\n",
        "6. FC-128\n",
        "7. FC-X\n",
        "\n",
        "Input: 28x28 JPEG\n",
        " \n",
        "In sum: 100 epochs per batch\n",
        "dataset division: 80% training, 20% testing\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdmEjur9-6zW",
        "cellView": "form"
      },
      "source": [
        "#@title Create model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# 1st layer\n",
        "# Convolution2D output data \n",
        "\n",
        "model.add(layers.Convolution2D( 64,\n",
        "          kernel_size=(3, 3),\n",
        "          strides=(1, 1),\n",
        "          padding='valid',\n",
        "          input_shape=(28, 28, 3),\n",
        "          activation='relu'\n",
        "          \n",
        "))\n",
        "# 2nd layer\n",
        "# MaxPooling2D output data \n",
        "\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 3rd layer\n",
        "# Convolution2D output data \n",
        "model.add(layers.Convolution2D( 128,\n",
        "          kernel_size=(3, 3),\n",
        "          strides=(1, 1),\n",
        "          padding='valid',\n",
        "          activation='relu'\n",
        "))\n",
        "# 4th layer\n",
        "# MaxPooling2D output data \n",
        "\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 5th layer\n",
        "# flatten output data \n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# 6th layer\n",
        "model.add(layers.Dense( 128, activation='relu'))\n",
        "\n",
        "# 7th layer\n",
        "model.add(layers.Dense( len(classes.keys()) ))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy']\n",
        ")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kms5nasOQnQo"
      },
      "source": [
        "## > Convolutional Model ++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH7a2D1rRkjt"
      },
      "source": [
        "```\n",
        "[Layers]\n",
        "1. Conv2D_64\n",
        "2. Maxpool2D, stride 2\n",
        "3. Conv2D_128\n",
        "4. Maxpool2D, stride 2\n",
        "3. Conv2D_256\n",
        "4. Maxpool2D, stride 2\n",
        "3. Conv2D_512\n",
        "4. Maxpool2D, stride 2\n",
        "5. Flatten\n",
        "6. FC-4096\n",
        "6. FC-128\n",
        "7. FC-X\n",
        " \n",
        "Input: 224x224 RGB JPEG\n",
        " \n",
        "In sum: 100 epochs per batch\n",
        "dataset division: 80% training, 20% testing\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N19rpSi3RjbX",
        "outputId": "b840007d-3392-401d-c2d6-35c5909145f5"
      },
      "source": [
        "#@title Create model\n",
        "model = tf.keras.models.Sequential(name=\"MODEL_BIQA_NASA\")\n",
        "\n",
        "# 1st layer\n",
        "# Convolution2D output data \n",
        "\n",
        "model.add(layers.Convolution2D( 64,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(1, 1),\n",
        "        padding='valid',\n",
        "        activation='relu',\n",
        "        name=\"Conv2D_64\",\n",
        "        input_shape=(None, 224, 224, 3),\n",
        "))\n",
        "# 2nd layer\n",
        "# MaxPooling2D output data \n",
        "\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), name=\"MaxPool2D\"))\n",
        "\n",
        "# 3rd layer\n",
        "# Convolution2D output data \n",
        "model.add(layers.Convolution2D( 128,\n",
        "          kernel_size=(3, 3),\n",
        "          strides=(1, 1),\n",
        "          padding='valid',\n",
        "          activation='relu',\n",
        "          name=\"Conv2D_128\"\n",
        "))\n",
        "# 4th layer\n",
        "# MaxPooling2D output data \n",
        "\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), name=\"MaxPool2D\"))\n",
        "\n",
        "# 5th layer\n",
        "# Convolution2D output data \n",
        "model.add(layers.Convolution2D( 256,\n",
        "          kernel_size=(3, 3),\n",
        "          strides=(1, 1),\n",
        "          padding='valid',\n",
        "          activation='relu',\n",
        "          name=\"Conv2D_256\"\n",
        "))\n",
        "# 6th layer\n",
        "# MaxPooling2D output data \n",
        "\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), name=\"MaxPool2D\"))\n",
        "\n",
        "# 7th layer\n",
        "# Convolution2D output data \n",
        "model.add(layers.Convolution2D( 512,\n",
        "          kernel_size=(3, 3),\n",
        "          strides=(1, 1),\n",
        "          padding='valid',\n",
        "          activation='relu',\n",
        "          name=\"Conv2D_512\"\n",
        "))\n",
        "# 8th layer\n",
        "# MaxPooling2D output data \n",
        "\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), name=\"MaxPool2D\"))\n",
        "\n",
        "# 9th layer\n",
        "# Convolution2D output data \n",
        "model.add(layers.Convolution2D( 1024,\n",
        "          kernel_size=(3, 3),\n",
        "          strides=(1, 1),\n",
        "          padding='valid',\n",
        "          activation='relu',\n",
        "          name=\"Conv2D_1024\"\n",
        "))\n",
        "# 10th layer\n",
        "# MaxPooling2D output data \n",
        "\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2),name=\"MaxPool2D\"))\n",
        "\n",
        "# 11th layer\n",
        "# flatten output data \n",
        "model.add(layers.Flatten(name=\"Flatten\"))\n",
        "\n",
        "# 12th layer\n",
        "model.add(layers.Dense( 4096, activation='relu', name=\"FC_4096\"))\n",
        "\n",
        "# 13th layer\n",
        "model.add(layers.Dense( 128, activation='relu', name=\"FC_128\"))\n",
        "\n",
        "# 14th layer\n",
        "model.add(layers.Dense( len(classes.keys()), name=f\"FC_{len(classes.keys())}\" ))\n",
        "\n",
        "# 15th layer\n",
        "model.add(layers.Softmax(name=\"Softmax\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy']\n",
        ")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwYCb3h7sGFM"
      },
      "source": [
        "# MODEL TRAINING AND EVALUATION\n",
        "\n",
        "Loading data in batches and fitting and evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy6OcuP8zz7K"
      },
      "source": [
        "## GUIDED WALKTHROUGH\n",
        "\n",
        "=== BATCH LOOP (in iter form) ==="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGXeCAR0C0Gp",
        "cellView": "form"
      },
      "source": [
        "#@title Create batch-iterator\n",
        "itbatch = iter(numpy_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vS1ED4PCyDi",
        "cellView": "form"
      },
      "source": [
        "#@title Take next file\n",
        "batch = next(itbatch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdEVmFi2iytE",
        "cellView": "form"
      },
      "source": [
        "#@title Load data to variables from file \n",
        "with np.load(os.path.join(base_path, batch)) as data_batch:\n",
        "    x = data_batch['dataset']\n",
        "    y = data_batch['labels']\n",
        "print(f\"FILE: {batch} => dataset: {x.shape}, labels: {y.shape}, classes: {classes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiMhfLLY_4z-"
      },
      "source": [
        "Load batch into shuffler to distinguish training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfm93wyeGSIz",
        "cellView": "form"
      },
      "source": [
        "#@title Make StratifiedKFold data divider and create iterator for folds\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "skfiter = skf.split(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM3djUH8wEdB",
        "cellView": "form"
      },
      "source": [
        "#@title Take next fold and split data into particular sets [training, testing]\n",
        "train_indices, test_indices = next(skfiter)\n",
        "\n",
        "x_train, x_test = x[train_indices], x[test_indices]\n",
        "y_train, y_test = y[train_indices], y[test_indices]\n",
        "print(f\"TRAIN: {train_indices[:10]} & TEST: {test_indices[:10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFD_nUzxIGXp",
        "cellView": "form"
      },
      "source": [
        "#@title Train model\n",
        "model.fit(x=x_train, y=y_train, epochs=20, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYcAhQmfIJir",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate model after training cycle\n",
        "score = model.evaluate(x_test, y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jauoudDtv2Oo"
      },
      "source": [
        "## ONE BLOCK TRAINING/VALIDATING LOOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh9ELHXwv6jO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "afd07fcb-00e9-48db-d60b-d1d6a4161330"
      },
      "source": [
        "#@title Start training\n",
        "    score = []\n",
        "    \n",
        "    print(banner(\"TRAINING\", length=default_line_length))\n",
        "    \n",
        "    start = time.time()\n",
        "    epoch_num = 16\n",
        "    for bnum, batch in enumerate(train_files):\n",
        "        batch_time = time.time()\n",
        "        with np.load(os.path.join(base_path, batch)) as data_batch:\n",
        "    # Loading from *.npz\n",
        "            x = data_batch['dataset']\n",
        "            y = data_batch['labels']\n",
        "        print(f\"┏━ FILE: {batch} :\\n┃ train_set:\\t{x.shape} » train_labels:\\t{y.shape}\")\n",
        "        print(\"┃ \"+\"_\"*63)\n",
        "    # Splitting for training and evaluation\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "        n = 0\n",
        "        for train_indices, test_indices in skf.split(x, y):    \n",
        "            x_train, x_test = x[train_indices], x[test_indices]\n",
        "            y_train, y_test = y[train_indices], y[test_indices]\n",
        "            n += 1\n",
        "    # Training..\n",
        "            training = time.time()\n",
        "            fit_loss, fit_accuracy = model.fit(x=x_train, y=y_train, batch_size=96, epochs=epoch_num, verbose=0)\n",
        "            print(f\"┃ ⤷Fold_{n} training ({epoch_num} epochs): {round(time.time() - training)} seconds..\")\n",
        "            print(f\"┃ \\t↻Fold_{n} training: {fit_loss=}, {fit_accuracy=}\")\n",
        "    # Evaluating..\n",
        "            eval_loss, eval_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "            print(f\"┃ \\t↺Fold_{n} evaluation: {eval_loss=:.2f}, {eval_accuracy=:.2f}\")\n",
        "            score.append({\n",
        "                \"fit_loss\": fit_loss,\n",
        "                \"fit_accuracy\": fit_accuracy,\n",
        "                \"eval_loss\": eval_loss,\n",
        "                \"eval_accuracy\": eval_accuracy\n",
        "            })\n",
        "    # Ending..\n",
        "        print(f'┗━ FILE: {batch} time: {time.strftime(\"%H h %M min %S sec\", time.gmtime(round(time.time() - batch_time)))}')\n",
        "        if tmp_save:\n",
        "            checkpoint = f\"{model_path}{bnum:02}\"\n",
        "            model.save(checkpoint)\n",
        "            print(f\"🗘 Model saved as: {checkpoint}\".center(default_line_length))\n",
        "        print(\"=\"*default_line_length)\n",
        "    # Summary time spent training and evaluating\n",
        "    print(f'⇶ Full training took: {time.strftime(\"%H h %M min %S sec\", time.gmtime(round(time.time() - start)))}')\n",
        "    \n",
        "    \"\"\" ~~~~ SAVE MODEL ~~~~ \"\"\"\n",
        "    model.save(model_path)\n",
        "    print(f\"⮔ Model saved as: {model_path}\\n\".center(default_line_length))\n",
        "\n",
        "    # Save score for plotting\n",
        "    with open(os.path.join(base_path, f'{model_name}_train_stats.json'), 'w') as fjson:\n",
        "        json.dump(score, fjson)\n",
        "\n",
        "    \"\"\" ~~~~ TEST MODEL'S ACCURACY ~~~~ \"\"\"\n",
        "    print(banner(\"TESTING\", length=default_line_length))\n",
        "\n",
        "    nasa_predictions = []\n",
        "    nasa_labels = []\n",
        "    nature_predictions = []\n",
        "    nature_labels = []\n",
        "\n",
        "    for test_file in test_files:\n",
        "    # Loading from *.npz\n",
        "        with np.load(os.path.join(base_path, test_file)) as test_batch:\n",
        "            nasa_dataset    = test_batch[\"nsdtest\"]\n",
        "            nature_dataset  = test_batch[\"ntdtest\"]\n",
        "    # Storing real labels\n",
        "            nasa_labels.extend(test_batch[\"nsltest\"])\n",
        "            nature_labels.extend(test_batch[\"ntltest\"])\n",
        "    # Predicting labels and storing\n",
        "            nasa_predictions.extend(model.predict(nasa_dataset))\n",
        "            nature_predictions.extend(model.predict(nature_dataset))\n",
        "          \n",
        "    # Save data for plotting\n",
        "    np.savez(os.path.join(base_path, f'{model_name}_stats.npz'),\n",
        "        nasa_predictions=nasa_predictions,\n",
        "        nasa_labels=nasa_labels,\n",
        "        nature_predictions=nature_predictions,\n",
        "        nature_labels=nature_labels\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgwg-rVEPORv"
      },
      "source": [
        "# SAVE / LOAD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19gRszayO48n",
        "cellView": "form",
        "outputId": "1319ab45-5cc1-44ce-b7dc-c7297f071eb0"
      },
      "source": [
        "#@title SAVE MODEL\n",
        "model_name = \"simple_fc_model\" #@param {type:\"string\"}\n",
        "model.save(os.path.join(models_path, model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Y81s5ZNOPTbH"
      },
      "source": [
        "#@title LOAD MODEL\n",
        "model_name = \"simple_fc_model\" #@param {type:\"string\"}\n",
        "model = keras.models.load_model(os.path.join(models_path, model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHb0w92SseP2"
      },
      "source": [
        "# MODEL TESTING\n",
        "\n",
        "We have now a model! Lets test its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6SV_JiSz5c7",
        "cellView": "form"
      },
      "source": [
        "#@title Load necessary functions to test model\n",
        "### Matplotlib functions to plot image\n",
        "import matplotlib.pyplot as plt\n",
        "classes_reversed = list(classes.keys())\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    true_label, img = true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(classes_reversed[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                classes_reversed[true_label]),\n",
        "                                color=color)\n",
        "  \n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "    true_label = true_label[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks(range(6))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(6), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVgVuWVvxbof"
      },
      "source": [
        "# with np.load(os.path.join(base_path, numpy_batches[0])) as data:\n",
        "#     x = data['dataset']\n",
        "#     y = data['labels']\n",
        "\n",
        "# prob_model = tf.keras.Sequential([model, layers.Softmax()])\n",
        "\n",
        "predictions = prob_model.predict(x)\n",
        "\n",
        "### see results\n",
        "i = random.randint(0, len(x)-1)\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], y, x)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  y)\n",
        "_ = plt.xticks(range(len(classes)), classes_reversed, rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}